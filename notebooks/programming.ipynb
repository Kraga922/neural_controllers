{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f5e166-38a4-4d7c-8b3f-2964ba367711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca061257-4ad9-47bf-bbe8-93d526ca9b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_path = Path().absolute()\n",
    "sys.path.append(str(notebook_path.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9d3f62-c301-43b0-801d-d8a5ceda91c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nc_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from neural_controllers import NeuralController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6438f5-0de8-48f5-843a-fac5dc46cb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import programming_language_dataset, pca_programming_language_dataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77eb18-a0d1-4892-a81c-58c04bc67820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.91s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 124.25 MiB is free. Process 1029251 has 26.31 GiB memory in use. Process 1031351 has 2.61 GiB memory in use. Process 1034368 has 30.32 GiB memory in use. Including non-PyTorch memory, this process has 19.86 GiB memory in use. Of the allocated memory 19.46 GiB is allocated by PyTorch, and 1.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type==\u001b[33m'\u001b[39m\u001b[33mllama\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m      4\u001b[39m     model_id = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     language_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     use_fast_tokenizer = \u001b[33m\"\u001b[39m\u001b[33mLlamaForCausalLM\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m language_model.config.architectures\n\u001b[32m     11\u001b[39m     tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=use_fast_tokenizer, padding_side=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m, legacy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nc_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    568\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nc_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4264\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4254\u001b[39m         load_contexts.append(tp_device)\n\u001b[32m   4256\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[32m   4257\u001b[39m         (\n\u001b[32m   4258\u001b[39m             model,\n\u001b[32m   4259\u001b[39m             missing_keys,\n\u001b[32m   4260\u001b[39m             unexpected_keys,\n\u001b[32m   4261\u001b[39m             mismatched_keys,\n\u001b[32m   4262\u001b[39m             offload_index,\n\u001b[32m   4263\u001b[39m             error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4264\u001b[39m         ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[32m   4268\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m            \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4277\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4278\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4279\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4280\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4281\u001b[39m \u001b[43m            \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4282\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4284\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4285\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nc_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4777\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[39m\n\u001b[32m   4773\u001b[39m                 set_module_tensor_to_device(\n\u001b[32m   4774\u001b[39m                     model_to_load, key, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, torch.empty(*param.size(), dtype=dtype)\n\u001b[32m   4775\u001b[39m                 )\n\u001b[32m   4776\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4777\u001b[39m         new_error_msgs, offload_index, state_dict_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4778\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4779\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4780\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4781\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4782\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4783\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4784\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4785\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4786\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4787\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4788\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4789\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4791\u001b[39m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4792\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4793\u001b[39m         error_msgs += new_error_msgs\n\u001b[32m   4794\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4795\u001b[39m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nc_env/lib/python3.11/site-packages/transformers/modeling_utils.py:942\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[39m\n\u001b[32m    939\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    941\u001b[39m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    944\u001b[39m     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nc_env/lib/python3.11/site-packages/accelerate/utils/modeling.py:337\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[39m\n\u001b[32m    335\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 124.25 MiB is free. Process 1029251 has 26.31 GiB memory in use. Process 1031351 has 2.61 GiB memory in use. Process 1034368 has 30.32 GiB memory in use. Including non-PyTorch memory, this process has 19.86 GiB memory in use. Of the allocated memory 19.46 GiB is allocated by PyTorch, and 1.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_type = 'llama'\n",
    "\n",
    "if model_type=='llama':\n",
    "    model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "    language_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, device_map=\"cuda\"\n",
    "    )\n",
    "\n",
    "    use_fast_tokenizer = \"LlamaForCausalLM\" not in language_model.config.architectures\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False)\n",
    "    tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "    model_name='llama_3_8b_it'\n",
    "    \n",
    "elif model_type=='gemma':\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "    language_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-9b-it\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model_name='gemma_2_9b_it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e2d56-b372-42a4-b116-fe8f7b4cc1f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concept_types = ['python', 'c++']\n",
    "concept_types = ['python', 'javascript']\n",
    "data_dir = \"../data/programming\"\n",
    "\n",
    "dataset = programming_language_dataset(concept_types, tokenizer)\n",
    "# dataset = pca_programming_language_dataset(concept_types, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c921a75-6641-4e17-9c51-162e0a1caa59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "controllers = {}\n",
    "for concept_type in tqdm(concept_types):\n",
    "    \n",
    "    other_type = [k for k in concept_types if k != concept_type][0]\n",
    "    \n",
    "    train_data = dataset[concept_type]['train']\n",
    "    test_data = dataset[concept_type]['test']\n",
    "        \n",
    "    language_controller = NeuralController(\n",
    "        language_model,\n",
    "        tokenizer,\n",
    "        rfm_iters=8,\n",
    "        batch_size=2,\n",
    "    )\n",
    "    \n",
    "    language_controller.compute_directions(train_data['inputs'], train_data['labels'])\n",
    "    \n",
    "    controllers[concept_type] = language_controller\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3cd2e-3b64-4e31-a980-aa0c32b25f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for concept_type in concept_types:\n",
    "    try:\n",
    "        controller = controllers[concept_type]\n",
    "        other_type = [k for k in concept_types if k!=concept_type][0]\n",
    "        controller.save(concept=f'{concept_type}_{other_type}', model_name=model_name, path='../directions/')\n",
    "    except:\n",
    "        print(f'{concept_type} not found')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f8cb2-3e56-46dd-905c-0b15f0e9421c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37229855-01cc-4770-a524-6d5432120786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "huggingface_dataset = load_dataset(\"greengerong/leetcode\")\n",
    "python_dataset = huggingface_dataset[\"train\"]['python']\n",
    "js_dataset = huggingface_dataset[\"train\"]['javascript']\n",
    "\n",
    "\n",
    "def extract_code(c):\n",
    "    items = c.split(\"```\")\n",
    "    code = items[1]\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311a72e-dc35-4e36-b77c-e805e598aa29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concept_types = ['python', 'javascript']\n",
    "controllers = {}\n",
    "\n",
    "for concept_type in concept_types:\n",
    "    \n",
    "    controller = NeuralController(\n",
    "        language_model,\n",
    "        tokenizer,\n",
    "        control_method='rfm',\n",
    "        n_components=1\n",
    "    )\n",
    "    \n",
    "    other_type = [k for k in concept_types if k!=concept_type][0]\n",
    "    \n",
    "    try:\n",
    "        controller.load(\n",
    "                        concept=f'{concept_type}_{other_type}', \n",
    "                        model_name=model_name, \n",
    "                        path='../directions/')\n",
    "        controllers[concept_type] = controller\n",
    "    except:\n",
    "        print(f'{concept_type} not found')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130e73f-3f60-4709-8514-fb334ec190e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concept_type = \"javascript\"\n",
    "# concept_type = \"python\"\n",
    "\n",
    "idx=0\n",
    "js_task = extract_code(js_dataset[idx])\n",
    "python_task = extract_code(python_dataset[idx])\n",
    "prompt = f\"Re-state the following program. \"\n",
    "# prompt = f\"Give a single, different re-writing of this program with the same function. \"\n",
    "# prompt += f\"The output will be judged by an expert in all programming languages. \"\n",
    "# prompt += f\"Do not include an explanation.\\n\\n```{python_task}```\"\n",
    "prompt += f\"Do not include an explanation.\\n\\n```{js_task}```\"\n",
    "\n",
    "# prompt = f\"Re-state the following program. Do not include an explanation. {python_task}.\"\n",
    "# prompt = f\"Give a single, different re-writing of this program with the same function. \"\n",
    "# prompt += f\"The output will be judged by an expert in all programming languages. \"\n",
    "# # prompt += f\"Do not include an explanation.\\n\\n```{python_task}```\"\n",
    "\n",
    "\n",
    "\n",
    "layer_id = list(range(-1, -31, -1))\n",
    "# layer_id = list(range(-1, -41, -1))\n",
    "language_controller = controllers[concept_type]\n",
    "num_new_tokens = 150\n",
    "\n",
    "inputs = language_controller.format_prompt(prompt)\n",
    "\n",
    "\n",
    "# rfm\n",
    "# coeff=9 # for javascript, gemma\n",
    "coeff=0.7 # for javascript, llama\n",
    "\n",
    "print(inputs)\n",
    "print(\"===== No Control =====\")\n",
    "gen1 = language_controller.generate(inputs, max_new_tokens=num_new_tokens, do_sample=False)\n",
    "print(gen1[len(inputs):])\n",
    "print()\n",
    "print(f\"===== + {concept_type} Control =====\")\n",
    "gen2 = language_controller.generate(inputs, layers_to_control=layer_id, control_coef=coeff, \n",
    "                            max_new_tokens=num_new_tokens, do_sample=False)\n",
    "print(gen2[len(inputs):])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83b0db-a937-49b2-9bb3-d8650fe5fd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
