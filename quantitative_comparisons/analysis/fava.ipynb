{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e5e6f1-c21e-44a2-b5a4-ba36de1698ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee41a3e1-584c-4d7b-b3aa-f4f76e25a0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a09de0-ba8b-41dd-9710-04817da1a2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b29eda-f5e8-4b66-a3af-b5de14025447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "NEURAL_CONTROLLERS_DIR = os.environ['NEURAL_CONTROLLERS_DIR']\n",
    "sys.path.append(NEURAL_CONTROLLERS_DIR)\n",
    "from neural_controllers import NeuralController\n",
    "from utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5e539-7bc7-4170-881b-a33a8c650621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the range of seeds and methods\n",
    "seeds = np.arange(5)\n",
    "methods = ['rfm', 'linear', 'logistic']\n",
    "model_name = 'llama_70_8b'\n",
    "# Initialize dictionaries to store validation and test results\n",
    "d_val = {method: {} for method in methods}\n",
    "d_test = {method: {} for method in methods}\n",
    "trivial_metrics_val = []\n",
    "trivial_metrics_test = []\n",
    "selected_test_metrics = {method: [] for method in methods}\n",
    "\n",
    "# Load data for each method\n",
    "for method in methods:\n",
    "    # Initialize lists to store results for each seed\n",
    "    for s in seeds:\n",
    "        # Paths for validation and test metrics\n",
    "        path_val = f'{NEURAL_CONTROLLERS_DIR}/results/fava_annotated_results/llama_3_8b_it_{method}_seed_{s}_val_metrics.pkl'\n",
    "        path_test = f'{NEURAL_CONTROLLERS_DIR}/results/fava_annotated_results/llama_3_8b_it_{method}_seed_{s}_test_metrics.pkl'\n",
    "        \n",
    "        # print(path_val, path_test)\n",
    "        # Check if files exist\n",
    "        if not os.path.exists(path_val):\n",
    "            print(f\"Validation file not found: {path_val}\")\n",
    "            continue\n",
    "        if not os.path.exists(path_test):\n",
    "            print(f\"Test file not found: {path_test}\")\n",
    "            continue\n",
    "            \n",
    "        # Load validation and test results\n",
    "        with open(path_val, 'rb') as f:\n",
    "            val_results = pickle.load(f)\n",
    "        with open(path_test, 'rb') as f:\n",
    "            test_results = pickle.load(f)\n",
    "        \n",
    "        # Get layer keys (excluding any non-layer keys)\n",
    "        layers_val = [k for k in val_results.keys() if isinstance(val_results[k], dict)]\n",
    "        layers_test = [k for k in test_results.keys() if isinstance(test_results[k], dict)]\n",
    "        \n",
    "        \n",
    "        # Collect per-layer metrics for validation\n",
    "        for key in layers_val:\n",
    "            if key not in d_val[method]:\n",
    "                d_val[method][key] = []\n",
    "            d_val[method][key].append(val_results[key])\n",
    "        \n",
    "        # Collect per-layer metrics for test\n",
    "        for key in layers_test:\n",
    "            if key not in d_test[method]:\n",
    "                d_test[method][key] = []\n",
    "            d_test[method][key].append(test_results[key])\n",
    "        \n",
    "        # Identify the layer with the highest validation accuracy\n",
    "        if layers_val:  # Check if there are any layers\n",
    "            best_layer = max(layers_val, key=lambda x: val_results[x]['acc'])\n",
    "            best_test_metrics = test_results.get(best_layer, {})\n",
    "            \n",
    "            # Store all metrics for the selected test layer\n",
    "            selected_test_metrics[method].append(best_test_metrics)\n",
    "\n",
    "# Compute average and standard deviation of selected test metrics\n",
    "average_selected_test_metrics = {}\n",
    "std_selected_test_metrics = {}\n",
    "for method in methods:\n",
    "    test_metrics_list = selected_test_metrics[method]\n",
    "    if test_metrics_list:\n",
    "        # Initialize dict for each metric\n",
    "        metrics_dict = {}\n",
    "        for metric in test_metrics_list[0].keys():  # Assuming all dicts have same metrics\n",
    "            values = [metrics[metric] for metrics in test_metrics_list]\n",
    "            metrics_dict[metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values)\n",
    "            }\n",
    "        average_selected_test_metrics[method] = metrics_dict\n",
    "    else:\n",
    "        average_selected_test_metrics[method] = {}\n",
    "\n",
    "# Print all metrics for each method\n",
    "print(\"\\nTest Metrics (Selected by Best Validation Layer):\")\n",
    "for method in methods:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    metrics_dict = average_selected_test_metrics[method]\n",
    "    for metric, values in metrics_dict.items():\n",
    "        print(f\"{metric}: {values['mean']:.4f} ± {values['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c9c1b-a29b-4d85-ab08-ed6494c67e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the range of methods\n",
    "methods = ['linear', 'rfm', 'logistic', 'rfm_linear', 'linear_rfm']\n",
    "# methods = ['rfm', 'linear']\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "d = {method: {} for method in methods}\n",
    "trivial_accs = []\n",
    "\n",
    "# Initialize dictionary to store agg results\n",
    "aggs_over_seeds = {method: [] for method in methods}\n",
    "\n",
    "# Load data for each method\n",
    "for method in methods:\n",
    "    # seeds = np.arange(5)  # Add the seeds you want to evaluate\n",
    "    \n",
    "    for s in seeds:\n",
    "        path = f'{NEURAL_CONTROLLERS_DIR}/results/fava_annotated_results/llama_3_8b_it_{method}_seed_{s}_test_metrics.pkl'\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"File not found: {path}\")\n",
    "            continue\n",
    "            \n",
    "        with open(path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "\n",
    "        \n",
    "        # Store linear_agg results - store the entire dictionary\n",
    "        if 'aggregation' in results:\n",
    "            aggs_over_seeds[method].append(results['aggregation'])\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"\\nAccuracy Metrics (Aggregated Over Seeds):\")\n",
    "for method in methods:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    \n",
    "    # Calculate and print metrics for all entries in linear_agg\n",
    "    if aggs_over_seeds[method]:\n",
    "        # Get all metric keys from the first result\n",
    "        metric_keys = aggs_over_seeds[method][0].keys()\n",
    "        \n",
    "        for metric in metric_keys:\n",
    "            # Extract this metric from all seeds\n",
    "            values = [result[metric] for result in aggs_over_seeds[method]]\n",
    "            mean = np.mean(values)\n",
    "            std = np.std(values)\n",
    "            print(f\"{metric}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27162763-2286-4223-9308-5d3101179c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a3eb71f",
   "metadata": {},
   "source": [
    "## Gemma 9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf1d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the range of seeds and methods\n",
    "seeds = np.arange(5)\n",
    "methods = ['rfm', 'linear', 'logistic']\n",
    "\n",
    "# Initialize dictionaries to store validation and test results\n",
    "d_val = {method: {} for method in methods}\n",
    "d_test = {method: {} for method in methods}\n",
    "trivial_metrics_val = []\n",
    "trivial_metrics_test = []\n",
    "selected_test_metrics = {method: [] for method in methods}\n",
    "\n",
    "# Load data for each method\n",
    "for method in methods:\n",
    "    # Initialize lists to store results for each seed\n",
    "    for s in seeds:\n",
    "        # Paths for validation and test metrics\n",
    "        path_val = f'{NEURAL_CONTROLLERS_DIR}/results/fava_annotated_results/gemma_2_9b_it_{method}_seed_{s}_val_metrics.pkl'\n",
    "        path_test = f'{NEURAL_CONTROLLERS_DIR}/results/fava_annotated_results/gemma_2_9b_it_{method}_seed_{s}_test_metrics.pkl'\n",
    "        \n",
    "        # print(path_val, path_test)\n",
    "        # Check if files exist\n",
    "        if not os.path.exists(path_val):\n",
    "            print(f\"Validation file not found: {path_val}\")\n",
    "            continue\n",
    "        if not os.path.exists(path_test):\n",
    "            print(f\"Test file not found: {path_test}\")\n",
    "            continue\n",
    "            \n",
    "        # Load validation and test results\n",
    "        with open(path_val, 'rb') as f:\n",
    "            val_results = pickle.load(f)\n",
    "        with open(path_test, 'rb') as f:\n",
    "            test_results = pickle.load(f)\n",
    "        \n",
    "        # Get layer keys (excluding any non-layer keys)\n",
    "        layers_val = [k for k in val_results.keys() if isinstance(val_results[k], dict)]\n",
    "        layers_test = [k for k in test_results.keys() if isinstance(test_results[k], dict)]\n",
    "        \n",
    "        # Store trivial metrics if they exist\n",
    "        if 'trivial_metrics' in val_results:\n",
    "            trivial_metrics_val.append(val_results['trivial_metrics'])\n",
    "        if 'trivial_metrics' in test_results:\n",
    "            trivial_metrics_test.append(test_results['trivial_metrics'])\n",
    "        \n",
    "        # Collect per-layer metrics for validation\n",
    "        for key in layers_val:\n",
    "            if key not in d_val[method]:\n",
    "                d_val[method][key] = []\n",
    "            d_val[method][key].append(val_results[key])\n",
    "        \n",
    "        # Collect per-layer metrics for test\n",
    "        for key in layers_test:\n",
    "            if key not in d_test[method]:\n",
    "                d_test[method][key] = []\n",
    "            d_test[method][key].append(test_results[key])\n",
    "        \n",
    "        # Identify the layer with the highest validation accuracy\n",
    "        if layers_val:  # Check if there are any layers\n",
    "            best_layer = max(layers_val, key=lambda x: val_results[x]['acc'])\n",
    "            best_test_metrics = test_results.get(best_layer, {})\n",
    "            \n",
    "            # Store all metrics for the selected test layer\n",
    "            selected_test_metrics[method].append(best_test_metrics)\n",
    "\n",
    "# Compute average and standard deviation of selected test metrics\n",
    "average_selected_test_metrics = {}\n",
    "std_selected_test_metrics = {}\n",
    "for method in methods:\n",
    "    test_metrics_list = selected_test_metrics[method]\n",
    "    if test_metrics_list:\n",
    "        # Initialize dict for each metric\n",
    "        metrics_dict = {}\n",
    "        for metric in test_metrics_list[0].keys():  # Assuming all dicts have same metrics\n",
    "            values = [metrics[metric] for metrics in test_metrics_list]\n",
    "            metrics_dict[metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values)\n",
    "            }\n",
    "        average_selected_test_metrics[method] = metrics_dict\n",
    "    else:\n",
    "        average_selected_test_metrics[method] = {}\n",
    "\n",
    "# Print all metrics for each method\n",
    "print(\"\\nTest Metrics (Selected by Best Validation Layer):\")\n",
    "for method in methods:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    metrics_dict = average_selected_test_metrics[method]\n",
    "    for metric, values in metrics_dict.items():\n",
    "        print(f\"{metric}: {values['mean']:.4f} ± {values['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d563dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the range of methods\n",
    "methods = ['linear', 'rfm', 'logistic', 'rfm_linear', 'linear_rfm']\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "d = {method: {} for method in methods}\n",
    "trivial_accs = []\n",
    "\n",
    "# Initialize dictionary to store agg results\n",
    "aggs_over_seeds = {method: [] for method in methods}\n",
    "\n",
    "# Load data for each method\n",
    "for method in methods:    \n",
    "    for s in seeds:\n",
    "        path = f'{NEURAL_CONTROLLERS_DIR}/results/fava_annotated_results/gemma_2_9b_it_{method}_seed_{s}_test_metrics.pkl'\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"File not found: {path}\")\n",
    "            continue\n",
    "            \n",
    "        with open(path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "            \n",
    "        # Store trivial accuracy if it exists\n",
    "        if 'trivial_acc' in results:\n",
    "            trivial_accs.append(results['trivial_acc'])\n",
    "        \n",
    "        # Store linear_agg results - store the entire dictionary\n",
    "        if 'aggregation' in results:\n",
    "            aggs_over_seeds[method].append(results['aggregation'])\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"\\nAccuracy Metrics (Aggregated Over Seeds):\")\n",
    "for method in methods:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    \n",
    "    # Calculate and print metrics for all entries in linear_agg\n",
    "    if aggs_over_seeds[method]:\n",
    "        # Get all metric keys from the first result\n",
    "        metric_keys = aggs_over_seeds[method][0].keys()\n",
    "        \n",
    "        for metric in metric_keys:\n",
    "            # Extract this metric from all seeds\n",
    "            values = [result[metric] for result in aggs_over_seeds[method]]\n",
    "            mean = np.mean(values)\n",
    "            std = np.std(values)\n",
    "            print(f\"{metric}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398bf53",
   "metadata": {},
   "source": [
    "## Judge models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb37ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0d2ec-1d87-44c0-a6bd-540893f56f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store accumulated metrics\n",
    "accumulated_metrics = {}\n",
    "\n",
    "# Iterate over seeds\n",
    "for seed in seeds:\n",
    "    with open(f'../fava_annotated_results/llama_gpt-4o_seed_{seed}_metrics.pkl', 'rb') as f:\n",
    "        llama_results = pickle.load(f)\n",
    "        \n",
    "        # Initialize accumulated_metrics with the first seed's metrics\n",
    "        if not accumulated_metrics:\n",
    "            accumulated_metrics = {metric: [] for metric in llama_results.keys()}\n",
    "            \n",
    "        # Accumulate metrics for each seed\n",
    "        for metric, val in llama_results.items():\n",
    "            accumulated_metrics[metric].append(val)\n",
    "\n",
    "# Calculate and print averages\n",
    "print('Llama (averaged over seeds)')\n",
    "print('-' * 30)\n",
    "for metric, values in accumulated_metrics.items():\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"{metric}: {mean_val:.4f} ± {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store accumulated metrics\n",
    "accumulated_metrics = {}\n",
    "\n",
    "# Iterate over seeds\n",
    "for seed in seeds:\n",
    "    with open(f'../fava_annotated_results/gemma_gpt-4o_seed_{seed}_metrics.pkl', 'rb') as f:\n",
    "        llama_results = pickle.load(f)\n",
    "        \n",
    "        # Initialize accumulated_metrics with the first seed's metrics\n",
    "        if not accumulated_metrics:\n",
    "            accumulated_metrics = {metric: [] for metric in llama_results.keys()}\n",
    "            \n",
    "        # Accumulate metrics for each seed\n",
    "        for metric, val in llama_results.items():\n",
    "            accumulated_metrics[metric].append(val)\n",
    "\n",
    "# Calculate and print averages\n",
    "print('Gemma (averaged over seeds)')\n",
    "print('-' * 30)\n",
    "for metric, values in accumulated_metrics.items():\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"{metric}: {mean_val:.4f} ± {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e72f4-f5b7-4a0c-af39-06602226e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store accumulated metrics\n",
    "accumulated_metrics = {}\n",
    "\n",
    "# Iterate over seeds\n",
    "for seed in seeds:\n",
    "    with open(f'../fava_annotated_results/openai_gpt-4o_seed_{seed}_metrics.pkl', 'rb') as f:\n",
    "        llama_results = pickle.load(f)\n",
    "        \n",
    "        # Initialize accumulated_metrics with the first seed's metrics\n",
    "        if not accumulated_metrics:\n",
    "            accumulated_metrics = {metric: [] for metric in llama_results.keys()}\n",
    "            \n",
    "        # Accumulate metrics for each seed\n",
    "        for metric, val in llama_results.items():\n",
    "            accumulated_metrics[metric].append(val)\n",
    "\n",
    "# Calculate and print averages\n",
    "print('GPT-4o (averaged over seeds)')\n",
    "print('-' * 30)\n",
    "for metric, values in accumulated_metrics.items():\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"{metric}: {mean_val:.4f} ± {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c7f3a",
   "metadata": {},
   "source": [
    "### Layer-wise plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137529c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the range of seeds\n",
    "seeds = np.arange(20)\n",
    "methods = ['rfm', 'logistic']\n",
    "models = ['llama_3_8b', 'gemma_2_9b']\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "all_results = {}\n",
    "\n",
    "for model in models:\n",
    "    all_results[model] = {\n",
    "        'test': {method: {} for method in methods}\n",
    "    }\n",
    "\n",
    "    # Load data for each method\n",
    "    for method in methods:\n",
    "        # Process each seed\n",
    "        for s in seeds:\n",
    "            # Path for test metrics\n",
    "            path_test = f'/u/dbeaglehole/mech_interp/neural_controllers/quantitative_comparisons/fava_annotated_results/{model}_it_{method}_seed_{s}_test_metrics.pkl'\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(path_test):\n",
    "                print(f\"File not found: {path_test}\")\n",
    "                continue\n",
    "                \n",
    "            # Load test results\n",
    "            with open(path_test, 'rb') as f:\n",
    "                test_results = pickle.load(f)\n",
    "            \n",
    "            # Get layer keys (excluding non-dict entries)\n",
    "            layers_test = [k for k in test_results.keys() if isinstance(test_results[k], dict)]\n",
    "            \n",
    "            # Collect per-layer metrics\n",
    "            for key in layers_test:\n",
    "                if key not in all_results[model]['test'][method]:\n",
    "                    all_results[model]['test'][method][key] = []\n",
    "                all_results[model]['test'][method][key].append(test_results[key]['acc'])\n",
    "\n",
    "# Calculate average accuracies per layer\n",
    "avg_results = {}\n",
    "for model in models:\n",
    "    avg_results[model] = {}\n",
    "    for method in methods:\n",
    "        avg_results[model][method] = {}\n",
    "        for layer, accs in all_results[model]['test'][method].items():\n",
    "            if accs:  # Check if we have data for this layer\n",
    "                avg_results[model][method][layer] = {\n",
    "                    'mean': np.mean(accs),\n",
    "                    'std': np.std(accs)\n",
    "                }\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Define colors and alpha values for shading\n",
    "colors = {'rfm': 'blue', 'logistic': 'red'}\n",
    "alpha_fill = 0.2  # Alpha transparency for the shaded region\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    \n",
    "    for method in methods:\n",
    "        layer_data = avg_results[model][method]\n",
    "        print(\"layer_data.keys()\", layer_data.keys())\n",
    "        if layer_data:\n",
    "            # Sort layers by their numerical value\n",
    "            layers = sorted([int(layer) for layer in layer_data.keys() if isinstance(layer, int)])\n",
    "            print(\"layers\", layers)\n",
    "            \n",
    "            # Extract mean and std for plotting\n",
    "            means = [layer_data[layer]['mean'] for layer in layers if layer in layer_data]\n",
    "            stds = [layer_data[layer]['std'] for layer in layers if layer in layer_data]\n",
    "            \n",
    "            # Plot the mean line\n",
    "            plt.plot(layers, means, label=method.upper(), marker='o', color=colors[method])\n",
    "            \n",
    "            # Add shaded error region\n",
    "            plt.fill_between(\n",
    "                layers, \n",
    "                [m - s for m, s in zip(means, stds)],  # lower bound\n",
    "                [m + s for m, s in zip(means, stds)],  # upper bound\n",
    "                color=colors[method], \n",
    "                alpha=alpha_fill\n",
    "            )\n",
    "    \n",
    "    plt.title(f'FAVA, Average Test Accuracies Across Layers for {model.replace(model[0], model[0].upper()).replace(\"_\", \"-\")}-it')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('fava_test_accuracies_across_layers.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daniel_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
